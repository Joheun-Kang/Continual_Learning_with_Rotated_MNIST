{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2_report.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFTS4OnXhgyU",
        "colab_type": "text"
      },
      "source": [
        "# CS6133_Project_2_Report <br>\n",
        "Joh Eun Kang (jk5726)<br>\n",
        "Dinesh Sreekanthan (ds5786)<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt_S6sM0hoWu",
        "colab_type": "text"
      },
      "source": [
        "# __Project Title:<br>__\n",
        "Gradient Episodic Momory with rotated MNIST data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGlgkmfaiBFf",
        "colab_type": "text"
      },
      "source": [
        "# __Motivation:__<br>\n",
        "In training machine learning model, we are often faced with difficulties of re-training the entire data every time we get updated data. This process would not be a big deal if the data is small enough to train all over again. However, since we often deal with large amounts of data, re-training would not be effiecent way to train the machine. <br>\n",
        "\n",
        "So, people came up with the new term called __\"Machine Intelligence\"__.<br>\n",
        " \n",
        "There are sigificant differences between machines and humans in terms of learning.  When a machine learns new data, they only remember the most current learning experience, not the history of their learning experiences. (previous task, current task). <br>\n",
        "\n",
        "However, when human learn new things, we still remember some of previous learning experinece. Even though we forget some of the old memories over time, but we don't forget the entire experience at once. Also, we often use those expeiences for our current or next learning more efficently!<br> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NktfFk2-Fh-4",
        "colab_type": "text"
      },
      "source": [
        "#__Project Purpose__:\n",
        "\n",
        "The goal of continual learning is to try applying such a person's learning method on the machines. There are several ways to to measure the performace of the model in terms of CL such as  measuring __backward transfer__ and __forward transfer__ rate. <br>\n",
        "\n",
        "__Backward transfer (BWT):__ is the influence that learning a current task has on the performance on a previous task.<br>\n",
        "-  positive backward transfer: when learning about some task t increases the performance on some preceding task k.<br>\n",
        "- negative backward transfer when learning about some task t decreases the performance on some preceding task k. (Large negative backward transfer: __catastrophic forgetting.__)<br>\n",
        "\n",
        "__Forward transfer(FWT):__ is the influence that learning a taskt has on the performance on a future task k ≻ t. <br>\n",
        "- positive forward transfer__: possible when the model is able to perform “zero-shot” learning, perhaps by exploiting the structure available in the task descriptors.<br>\n",
        "\n",
        "In this project, we focused more on understading backward transfer, and more specifically  __\"Catastrophic forgetting\"__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW1u0ttrhoUg",
        "colab_type": "text"
      },
      "source": [
        "# __Alrogithm Used:__ Gradient Episodic Memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyWhJ8RwiIFb",
        "colab_type": "text"
      },
      "source": [
        "#__Procedure:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSsiZYshoSV",
        "colab_type": "text"
      },
      "source": [
        "## 1. MNIST data set<br>\n",
        "The MNIST database, known as Modified National Institute of Standards and Technology database, is a large database of handwritten digits. We take a look at a variation of MNIST known as *mnist-rot*, where the digits are rotated by an angle between 0 and 2π. This means the important factor involved is the rotation angle.<br><br> \n",
        "\n",
        "\n",
        " In this section, we are going to see the actual image of data set and how the image is rotated. Rotation angle depends on how we set the angle in the 3rd cell of this section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT59yhd-jZQJ",
        "colab_type": "text"
      },
      "source": [
        "### - Download the data \n",
        "```python\n",
        "mnist_path = \"mnist.npz\"\n",
        "\n",
        "if not os.path.exists(mnist_path):\n",
        "    subprocess.call(\"wget https://s3.amazonaws.com/img-datasets/mnist.npz\", shell=True)\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "f = np.load('mnist.npz')\n",
        "x_tr = torch.from_numpy(f['x_train'])\n",
        "y_tr = torch.from_numpy(f['y_train']).long()\n",
        "x_te = torch.from_numpy(f['x_test'])\n",
        "y_te = torch.from_numpy(f['y_test']).long()\n",
        "f.close()\n",
        "\n",
        "torch.save((x_tr, y_tr), 'mnist_train.pt')\n",
        "torch.save((x_te, y_te), 'mnist_test.pt')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVCvCQVJjdee",
        "colab_type": "text"
      },
      "source": [
        "### - Data rotation function\n",
        "```python\n",
        "def rotate_dataset(d, rotation):\n",
        "    result = torch.FloatTensor(d.size(0), 784)\n",
        "    tensor = transforms.ToTensor()\n",
        "\n",
        "    for i in range(d.size(0)):\n",
        "        img = Image.fromarray(d[i].numpy(), mode='L')\n",
        "        result[i] = tensor(img.rotate(rotation)).view(784)\n",
        "    return result\n",
        "```\n",
        "since each image is size of (28,28), we are __flatten the pixel to 28*28 = 784__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PNB-J30juxb",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "n_tasks = 20  #number of tasks,\n",
        "min_rot = 0  #minimum rotation,\n",
        "max_rot = 180 #maximum rotation\n",
        "seed = 0  #random seed\n",
        "\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "\n",
        "x_tr, y_tr = torch.load('/content/mnist_train.pt') #data is tuple\n",
        "x_te, y_te = torch.load('/content/mnist_test.pt')\n",
        "\n",
        "for t in range(n_tasks):\n",
        "    min_rot = 1.0 * t / n_tasks * (max_rot - min_rot) + \\\n",
        "        min_rot\n",
        "    max_rot = 1.0 * (t + 1) / n_tasks * \\\n",
        "        (max_rot - min_rot) + min_rot\n",
        "    rot = random.random() * (max_rot - min_rot) + min_rot\n",
        "\n",
        "    tasks_tr.append([rot, rotate_dataset(x_tr, rot), y_tr])\n",
        "    tasks_te.append([rot, rotate_dataset(x_te, rot), y_te])\n",
        "\n",
        "torch.save([tasks_tr, tasks_te], 'mnist_rotations.pt') # make 'mnist_rotation.pt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn8M6Y3Yj260",
        "colab_type": "text"
      },
      "source": [
        "### - Rotated data visualization\n",
        "\n",
        "Structure of tasks (task1,,,,task20) are  <br>\n",
        "train = [[angle1,image(60000), label],[angle2,image(60000), label],,,,[angle20,image(60000), label]]<br>\n",
        "test =[[angle1,image(60000), label],[angle2,image(60000), label],,,,[angle20,image(60000), label]]<br>\n",
        "\n",
        "```python\n",
        "mrotation = torch.load('/content/mnist_rotations.pt')\n",
        "\n",
        "# first image of 1st task\n",
        "tensor_matrix_0 = mrotation[0][0][1]\n",
        "# first image of 2nd task\n",
        "tensor_matrix_1 = mrotation[0][1][1]\n",
        "# first image of 3rd task\n",
        "tensor_matrix_2 = mrotation[0][2][1]\n",
        "# first image of 4th task\n",
        "tensor_matrix_3 = mrotation[0][3][1]\n",
        "\n",
        "plt.figure()\n",
        "f, axarr = plt.subplots(1,4,figsize=(15, 10)) \n",
        "axarr[0].imshow(tensor_matrix_0[0].reshape(28,28), cmap=\"gray\")\n",
        "axarr[1].imshow(tensor_matrix_1[0].reshape(28,28), cmap=\"gray\")\n",
        "axarr[2].imshow(tensor_matrix_2[0].reshape(28,28), cmap=\"gray\")\n",
        "axarr[3].imshow(tensor_matrix_3[0].reshape(28,28), cmap=\"gray\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLpvpUsTkIN4",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZvnLnmmkSie",
        "colab_type": "text"
      },
      "source": [
        "As we can see from above pictures, __pictures are rotated!__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh1Vzr1CimFw",
        "colab_type": "text"
      },
      "source": [
        "## 2. Understanding Math behind the  GEM <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo3n7IxCelBF",
        "colab_type": "text"
      },
      "source": [
        "*@ David Lopez-Paz and Marc’Aurelio Ranzato\n",
        "Facebook Artificial Intelligence Research* <br>\n",
        "\n",
        "pesudo code for Training a GEM over an ordered continuum of data\n",
        "\n",
        "```python\n",
        "TRAIN(fθ,Continuumtrain,Continuumtest) \n",
        "Mt ←{}forallt=1,...,T.\n",
        "for (x, y) in Continuumtrain(t) do Mt←Mt∪(x,y) g←∇θl(fθ(x,t),y)\n",
        "gk ← ∇θ l(fθ, Mk) for all k < t\n",
        "g ̃ ← PROJECT(g, g1, . . . , gt−1), see (11). θ ← θ − αg ̃.\n",
        "end for\n",
        "Rt,: ← EVALUATE(fθ,Continuumtest) end for\n",
        "return fθ , R end procedure\n",
        "\n",
        "\n",
        "EVALUATE(fθ,Continuum) \n",
        "r←0∈RT\n",
        "R ← 0 ∈ R\n",
        "for t = 1,...,T do:\n",
        "T×T\n",
        "for\n",
        "k = 1, . . . , T do\n",
        "rk ← 0\n",
        "for (x, y) in Continuum(k) do\n",
        "rk ←rk+accuracy(fθ(x,k),y) end for\n",
        ".\n",
        "rk ← rk / len(Continuum(k)) end for\n",
        "return r end procedure\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vySaNW7ezOfC",
        "colab_type": "text"
      },
      "source": [
        "This procedure requires some optimization solving skills.<br>\n",
        "\n",
        "Code in cell below is the actual code that uses __quadratic programming solving__. <br>\n",
        "\n",
        "In this part, we are focusing math behind it .<br>\n",
        "\n",
        "As we mentioned in the inroduction, we are focusing on the positive backward transformation. So, we use inequality contrainst of optimization. The inequality contraint will avoid increasing the loss but allowing their decrease. (minimizing loss, subject to some contraints). <br>\n",
        "\n",
        "$minimize_θ \\ \\ L(f_θ (x, t), y)$<br>\n",
        "subject to    $L(fθ, M_k) ≤ L(f_{t−1}, M_k)$ for all $k < t$ <br>\n",
        "\n",
        "$f_{θ}^{t-1}$ is the predictor state at the end of learning of task $t − 1$.<br><br>\n",
        "\n",
        "\n",
        "\n",
        "From this equation, we know that if the preceding taks loss does not increase after the parameter update, we do not need to store old predictors. \n",
        "\n",
        "Also, we assume that the function is locally linear and the memory is representative of the examples from past tasks, we know the increases of loss of previous tasks by computing the angle between thier loss gradient vector and update it.<br>\n",
        "\n",
        "$<g,g_k> := < dL(f_{\\theta}(x,t),y)/d\\theta , dL(f_{theta},M_k)/d\\theta> \\ \\geq \\ 0$ for all $k,t$ <br>\n",
        "\n",
        "If the inequality constraints are satisfied, it implies the proposed parameter update $g$ is not likely to increase the loss at the previous tasks. However, if at least one of those constraints are violated, then there is at least one previous task that would experience an increase in loss after the parameter update. <br>\n",
        "\n",
        "This is same as solving quadratic programming <br>\n",
        "\n",
        "\n",
        "$minimize_{g ̃} 1/2 \\  ∥g − g ̃∥_2^2 $<br>\n",
        "subject to $⟨g ̃,g_k⟩ ≥ 0$  for all $k < t.$<br>\n",
        "$g$ = proposed gradient<br>\n",
        "$g~$ = closest gradient<br>\n",
        "\n",
        "$minimize (1/2)z⊤Cz + p⊤z$<br>\n",
        "subject to $Az ≥ b$,<br>  \n",
        "where $C \\in R^{p*p}, p \\in R^p, A \\in R^{(t-1)*p}, and b \\in R^{t-1}$ <br>\n",
        "\n",
        "Then, we need to make the primal to  dual problem and solve the quadratic programming. <br>\n",
        "\n",
        "$minimize_{u,v} (1/2)*u^TCu-b^Tv$<br>\n",
        "$subject to A^Tv-Cu = p$<br>\n",
        "$v \\geq 0$\n",
        "\n",
        "Assume that (u^*,v^*) is the optimal solution for above equation. Then ehre is a optimal z^* that satisfying Cz^* = Cu^*.<br>\n",
        "\n",
        "Then, we can write the primal GEM quadratic programming equation <br>\n",
        "$minimize (1/2)z⊤z − g⊤z + (1/2)g⊤g$ <br>\n",
        "$subject to Gz \\geq 0$ <br> \n",
        "$where G = 0(g_1,...,g_{t-1}$.<br>\n",
        "\n",
        "Since the $g^Tg$ is constant, we can remove it.\n",
        "\n",
        "\n",
        "__Finally__,<br>\n",
        "we get <br>\n",
        "\n",
        "$minimize (1/2)v^⊤GG^⊤v + g^⊤G^⊤v$ <br>\n",
        "$subject \\ to \\ v \\geq 0,$ <br>\n",
        "\n",
        "here, $u = G^Tv$ +g and $g^Tg$ is constant, and this is quadratic programming on the number of observed tasks so far. Once we solve this dual problem for $v^*$, we recover the projected gradient update $g ̃= G^Tv^* + g$.<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtzmGSHQhoNq",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
        "    memories_np = memories.cpu().t().double().numpy()\n",
        "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
        "    t = memories_np.shape[0]\n",
        "    P = np.dot(memories_np, memories_np.transpose())\n",
        "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
        "    q = np.dot(memories_np, gradient_np) * -1\n",
        "    G = np.eye(t)\n",
        "    h = np.zeros(t) + margin\n",
        "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
        "    x = np.dot(v, memories_np) + gradient_np\n",
        "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IhwCkjUEwtO",
        "colab_type": "text"
      },
      "source": [
        "In the below cell (part of gem.py), we see the __constraint procedure__ that is exaplained in above cell.\n",
        "\n",
        "Gem.py <br>\n",
        "class Net(nn.Module):\n",
        "```python\n",
        "        # compute gradient on previous tasks\n",
        "        if len(self.observed_tasks) > 1:\n",
        "            for tt in range(len(self.observed_tasks) - 1):\n",
        "                self.zero_grad()\n",
        "                # fwd/bwd on the examples in the memory\n",
        "                past_task = self.observed_tasks[tt]\n",
        "\n",
        "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task)\n",
        "                ptloss = self.ce(\n",
        "                    self.forward(\n",
        "                        self.memory_data[past_task],\n",
        "                        past_task)[:, offset1: offset2],\n",
        "                    self.memory_labs[past_task] - offset1)\n",
        "                ptloss.backward()\n",
        "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
        "                           past_task)\n",
        "\n",
        "        # now compute the grad on the current minibatch\n",
        "        self.zero_grad()\n",
        "\n",
        "        offset1, offset2 = compute_offsets(t, self.nc_per_task)\n",
        "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
        "        loss.backward()\n",
        "\n",
        "        # check if gradient violates constraints\n",
        "        if len(self.observed_tasks) > 1:\n",
        "            # copy gradient\n",
        "            store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
        "            indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
        "                else torch.LongTensor(self.observed_tasks[:-1])\n",
        "            dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
        "                            self.grads.index_select(1, indx))\n",
        "            if (dotp < 0).sum() != 0:\n",
        "                project2cone2(self.grads[:, t].unsqueeze(1),\n",
        "                              self.grads.index_select(1, indx), self.margin)\n",
        "                # copy gradients back\n",
        "                overwrite_grad(self.parameters, self.grads[:, t],\n",
        "                               self.grad_dims)\n",
        "        self.opt.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVmZ5FSVDHTH",
        "colab_type": "text"
      },
      "source": [
        "# __Architectures__<br>\n",
        "\n",
        "For the MNIST Rotated dataset, we use __fully- connected neural networks__ with __two hidden layers of 100 units of Relu__. Also, this model trains all the networks and baselines using __SGD__ wth __10 samples of mini baches__. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-zK-SgphoP9",
        "colab_type": "text"
      },
      "source": [
        "## 2. Understanding Code (Models & Architectures)\n",
        "In this part, we will expain the GEM architecture with some codes in Common.py and gem.py. Our explanation in this part may go back and forth with those codes. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYiiPzh_Ch_i",
        "colab_type": "text"
      },
      "source": [
        "In Common.py, we see functions below\n",
        "```python\n",
        "def Xavier(m):\n",
        "    if m.__class__.__name__ == 'Linear':\n",
        "        fan_in, fan_out = m.weight.data.size(1), m.weight.data.size(0)\n",
        "        std = 1.0 * math.sqrt(2.0 / (fan_in + fan_out))\n",
        "        a = math.sqrt(3.0) * std\n",
        "        m.weight.data.uniform_(-a, a)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        for i in range(0, len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
        "            if i < (len(sizes) - 2):\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.net.apply(Xavier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyuBuUA_HVQm",
        "colab_type": "text"
      },
      "source": [
        "In the __Xavior function__, we are __initializing weight with uniform distribution__. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwLbf5wpCiac",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = relu(out)\n",
        "        return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWH1SGFgHv_x",
        "colab_type": "text"
      },
      "source": [
        "We have two hidden layer with size __3x3  two convolutional layers__ with the __relu function__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXoebMthMDp0",
        "colab_type": "text"
      },
      "source": [
        "Then, in Gem.py, we can see the important part of code relate to the __gradient update__.(below code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYze1cPxLU62",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def store_grad(pp, grads, grad_dims, tid):\n",
        "    grads[:, tid].fill_(0.0)\n",
        "    cnt = 0\n",
        "    for param in pp():\n",
        "        if param.grad is not None:\n",
        "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
        "            en = sum(grad_dims[:cnt + 1])\n",
        "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
        "        cnt += 1\n",
        "\n",
        "\n",
        "def overwrite_grad(pp, newgrad, grad_dims):\n",
        "    cnt = 0\n",
        "    for param in pp():\n",
        "        if param.grad is not None:\n",
        "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
        "            en = sum(grad_dims[:cnt + 1])\n",
        "            this_grad = newgrad[beg: en].contiguous().view(\n",
        "                param.grad.data.size())\n",
        "            param.grad.data.copy_(this_grad)\n",
        "        cnt += 1\n",
        "\n",
        "```\n",
        "We calculate the gradient and store the previous gradient using store_grad function. Then, as we explained in the previous part, we use the inequality constraint of quadratic programming to determine whether we overwrite the gradient (violation) or maintain the previous gradient(if current loss is less than the previous).<br>\n",
        " We use a number of other auxilary functions and classes in gem.py which are outlined briefly as follows.<br>\n",
        "\n",
        "#Auxilary Functions\n",
        "**store_grad()**<br><br>\n",
        "This functions is used to store parameter gradients of previous tasks. A list for gradient stores task numbers. For each line, the previous parameters per layer is iteratively stored.<br><br>\n",
        "\n",
        "**overwrite_grad()**<br>\n",
        "Whenever a violation occurs.this function is used to overwrite the gradients with a new updated gradient vector with the count of previous parameters.<br><br>\n",
        "\n",
        "**project2cone2()**<br>\n",
        "\n",
        "The GEM dual Quadratic Program is solved by minimizing the gradient function in order to ensure that loss in previously saved memory does not occur when the inequality constraints are violated. It is solved by overwriting the old gradient with the final projected update.<br><br>\n",
        "\n",
        "#Neural Network Class\n",
        "**Class Net(nn.module)**<br>\n",
        "This class contains definitions for forward pass and backward pass as well as correcting and updating the gradient vector if any violations are found. It performs the main function of storing previous tasks of gradients and compares it to the current minibatch to check if any constraints were violated. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA8QRzLF8pra",
        "colab_type": "text"
      },
      "source": [
        "# __Evaluation Metrics__<br>\n",
        "\n",
        "To evaluate the model, we use three different metrics to test the ability of the model to not only be accurate but also test the transfer capability. The three metrics are average accuracy, backward transfer and forward transfer. It is important to evaluate the model's ability to transfer and forget, so instead of just measuring the average performance over all the tasks, we use a random seed in the form of mini batches in sequential ordering. These mini batches are composed of triplets and the average accuracy is measured as the average of the performance of all of these mini batches.<br><br>\n",
        "**Backward Transfer** is the influence that a learning task has on the previous task. If there is negative performance on the previous task, then it is said that *Catastrophic Forgetting* has occured.<br>\n",
        "**Forward Transfer** is the influence that a learning task has on a future task.<br>\n",
        "In general, it is favorable to have a model have high average accuracy, forward transfer and backward transfer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcU8VGDhhoLW",
        "colab_type": "text"
      },
      "source": [
        "#__Result__<br>\n",
        " We can see that we achieve high accuracy using GEM algorithm because it optimizes with respect to the number of tasks as opposed to conventional learning algorithms which optimize with respect to the number of parameters. The disadvantage is that at each training iteration, the algorithm needs to recompute the previous gradient vectors. The accuracy is calculated using a confusion matrix.\n",
        "\n",
        "Since we use random seed, the result may differ for each run. When you run the code, you will see a plot of the ACC, BTW and FWT metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l46epu5Kb0fL",
        "colab_type": "text"
      },
      "source": [
        "0.1111 0.1144 0.1196 0.1188 0.0947 0.0908 0.0851 0.0768 0.0712 0.0696 0.0692 0.0517 0.0506 0.0502 0.0510 0.0545 0.0603 0.0672 0.0714 0.0766\n",
        "|<br>\n",
        "0.7235 0.6779 0.5820 0.4097 0.3136 0.2513 0.1638 0.1466 0.1171 0.1145 0.1133 0.1210 0.1251 0.1289 0.1513 0.1797 0.1884 0.2029 0.2118 0.2152\n",
        "0.8492 0.8642 0.7903 0.5949 0.4642 0.3660 0.2518 0.2318 0.1725 0.1443 0.1435 0.1129 0.1117 0.1135 0.1306 0.1723 0.1839 0.1995 0.2367 0.2503\n",
        "0.8326 0.8808 0.8754 0.7915 0.6815 0.5529 0.3244 0.2716 0.1655 0.1413 0.1404 0.1237 0.1276 0.1250 0.1341 0.1609 0.1646 0.1766 0.1876 0.2003\n",
        "0.7759 0.8543 0.8708 0.8521 0.7849 0.7035 0.4593 0.4011 0.2077 0.1342 0.1328 0.0934 0.0912 0.0894 0.0987 0.1166 0.1235 0.1354 0.1762 0.1909\n",
        "0.7782 0.8470 0.8682 0.8622 0.8285 0.7547 0.5569 0.5051 0.3089 0.2082 0.2094 0.1281 0.1236 0.1158 0.1047 0.1200 0.1239 0.1348 0.1744 0.1898\n",
        "0.7669 0.8249 0.8408 0.8633 0.8643 0.8514 0.7392 0.6727 0.4393 0.2870 0.2847 0.1798 0.1628 0.1508 0.1402 0.1419 0.1486 0.1508 0.1934 0.2077\n",
        "0.7548 0.8096 0.8354 0.8525 0.8621 0.8490 0.8165 0.7849 0.6113 0.4618 0.4606 0.2389 0.1973 0.1762 0.1438 0.1312 0.1335 0.1263 0.1495 0.1643\n",
        "0.7728 0.8169 0.8373 0.8605 0.8662 0.8770 0.8733 0.8591 0.7512 0.5901 0.5828 0.3169 0.2602 0.2241 0.1615 0.1472 0.1445 0.1444 0.1591 0.1713\n",
        "0.7989 0.8424 0.8610 0.8715 0.8666 0.8761 0.8799 0.8728 0.8532 0.7764 0.7776 0.5323 0.4412 0.3802 0.2374 0.1874 0.1830 0.1657 0.1736 0.1793\n",
        "0.8071 0.8597 0.8804 0.8946 0.8968 0.8959 0.8998 0.8979 0.8941 0.8581 0.8555 0.6617 0.5649 0.4991 0.3297 0.2501 0.2352 0.1938 0.1916 0.2002\n",
        "0.7820 0.8381 0.8559 0.8617 0.8598 0.8627 0.8479 0.8502 0.8491 0.8246 0.8261 0.6654 0.5830 0.5244 0.3332 0.2282 0.2167 0.1810 0.1799 0.1929\n",
        "0.7663 0.8260 0.8556 0.8730 0.8715 0.8734 0.8729 0.8715 0.8860 0.9002 0.8999 0.8674 0.8235 0.7834 0.6086 0.4076 0.3675 0.2779 0.2076 0.2160\n",
        "0.7816 0.8419 0.8628 0.8811 0.8809 0.8845 0.8880 0.8844 0.8915 0.9008 0.9005 0.8875 0.8729 0.8457 0.6978 0.4842 0.4360 0.3233 0.2181 0.2181\n",
        "0.7141 0.7601 0.7687 0.7943 0.7936 0.8067 0.8207 0.8299 0.8505 0.8643 0.8634 0.8787 0.8620 0.8482 0.7410 0.5703 0.5356 0.4059 0.2391 0.2111\n",
        "0.7138 0.7928 0.8283 0.8723 0.8811 0.8889 0.8917 0.8878 0.8861 0.8943 0.8946 0.9087 0.9076 0.9027 0.8652 0.7195 0.6798 0.5287 0.2554 0.1985\n",
        "0.7489 0.8113 0.8424 0.8536 0.8515 0.8575 0.8567 0.8555 0.8534 0.8497 0.8498 0.8673 0.8705 0.8686 0.8647 0.8170 0.7984 0.7089 0.4387 0.3494\n",
        "0.7801 0.8440 0.8654 0.8789 0.8797 0.8863 0.8847 0.8853 0.8851 0.8893 0.8878 0.9025 0.9057 0.9082 0.9072 0.8821 0.8765 0.8179 0.5995 0.4608\n",
        "0.7943 0.8509 0.8693 0.8791 0.8788 0.8816 0.8788 0.8768 0.8842 0.8892 0.8909 0.8987 0.9005 0.9019 0.9079 0.9090 0.9055 0.8832 0.7342 0.5938\n",
        "0.6680 0.7436 0.7710 0.7821 0.7857 0.7943 0.7961 0.7959 0.8149 0.8270 0.8270 0.8306 0.8341 0.8388 0.8499 0.8665 0.8626 0.8679 0.8477 0.7791\n",
        "0.7324 0.8109 0.8417 0.8515 0.8585 0.8620 0.8651 0.8587 0.8658 0.8705 0.8699 0.8844 0.8853 0.8892 0.8864 0.8867 0.8868 0.8796 0.8573 0.8306\n",
        "\n",
        "Final Accuracy: 0.8587\n",
        "Backward: 0.0128\n",
        "Forward:  0.6594"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35CoXBI-heL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}